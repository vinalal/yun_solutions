# -*- coding: utf-8 -*-
"""web_scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mfH4vafbVpFvEX2iTfjvyxCGoUElj3Sv
"""

import requests
from bs4 import BeautifulSoup

# Function to scrape headlines from <h1> and <h2> tags from a given URL
def scrape_headlines(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            # Extract headlines from <h1> and <h2> tags
            h1_headlines = [h1.text.strip() for h1 in soup.find_all('h1')]
            h2_headlines = [h2.text.strip() for h2 in soup.find_all('h2')]
            return h1_headlines, h2_headlines
        else:
            print(f"Failed to fetch {url}. Status code: {response.status_code}")
            return None, None
    except Exception as e:
        print(f"Error scraping {url}: {str(e)}")
        return None, None

# Read URLs from the text file
urls_file = "urls.txt"  # Path to the text file containing URLs
with open(urls_file, 'r') as file:
    urls = file.read().splitlines()

# Create a dictionary to store scraped data
scraped_data = {}

# Scrape headlines from <h1> and <h2> tags from each URL and store them in the dictionary
for url in urls:
    h1_headlines, h2_headlines = scrape_headlines(url)
    if h1_headlines and h2_headlines:
        scraped_data[url] = {"h1_headlines": h1_headlines, "h2_headlines": h2_headlines}

# Output the scraped data
for source, data in scraped_data.items():
    print(f"Source: {source}")
    print("H1 Headlines:")
    for idx, headline in enumerate(data["h1_headlines"], start=1):
        print(f"{idx}. {headline}")
    print("H2 Headlines:")
    for idx, headline in enumerate(data["h2_headlines"], start=1):
        print(f"{idx}. {headline}")
    print()